{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou_bqgOmDx08"
      },
      "source": [
        "# Downloading the pretrained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "sys.path.append(\"../\")\n",
        "pretrained_model_dir = \"./pretrained_models\"\n",
        "\n",
        "if not os.path.exists(pretrained_model_dir):\n",
        "    os.mkdir(pretrained_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbygligUC4O5",
        "outputId": "d2a4e715-e4c3-4476-a4d5-591a12dd7482"
      },
      "outputs": [],
      "source": [
        "! cd pretrained_models && wget \"http://vllab1.ucmerced.edu/~yli62/CartoonGAN/pytorch_pth/Hayao_net_G_float.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-na75UbrEkZQ",
        "outputId": "e531a267-f1c6-4ae3-8844-f9541b1530f2"
      },
      "outputs": [],
      "source": [
        "! cd pretrained_models && wget \"http://vllab1.ucmerced.edu/~yli62/CartoonGAN/pytorch_pth/Hosoda_net_G_float.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cpMvqIBEmr5",
        "outputId": "7881ba02-ca99-4474-f9f3-426cc411bb50"
      },
      "outputs": [],
      "source": [
        "! cd pretrained_models && wget \"http://vllab1.ucmerced.edu/~yli62/CartoonGAN/pytorch_pth/Paprika_net_G_float.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR6xvdJJE_f7",
        "outputId": "8e449afd-cc14-419c-f1e7-408bea5c0607"
      },
      "outputs": [],
      "source": [
        "! cd pretrained_models && wget \"http://vllab1.ucmerced.edu/~yli62/CartoonGAN/pytorch_pth/Shinkai_net_G_float.pth\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQKSPFVUL7Sq"
      },
      "source": [
        "## Our transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uDBw4oTDLORc"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torchvision.utils as vutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1Dll-Wo9L2zG"
      },
      "outputs": [],
      "source": [
        "# networks/Transformer.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Transformer, self).__init__()\n",
        "        #\n",
        "        self.refpad01_1 = nn.ReflectionPad2d(3)\n",
        "        self.conv01_1 = nn.Conv2d(3, 64, 7)\n",
        "        self.in01_1 = InstanceNormalization(64)\n",
        "        # relu\n",
        "        self.conv02_1 = nn.Conv2d(64, 128, 3, 2, 1)\n",
        "        self.conv02_2 = nn.Conv2d(128, 128, 3, 1, 1)\n",
        "        self.in02_1 = InstanceNormalization(128)\n",
        "        # relu\n",
        "        self.conv03_1 = nn.Conv2d(128, 256, 3, 2, 1)\n",
        "        self.conv03_2 = nn.Conv2d(256, 256, 3, 1, 1)\n",
        "        self.in03_1 = InstanceNormalization(256)\n",
        "        # relu\n",
        "\n",
        "        # res block 1\n",
        "        self.refpad04_1 = nn.ReflectionPad2d(1)\n",
        "        self.conv04_1 = nn.Conv2d(256, 256, 3)\n",
        "        self.in04_1 = InstanceNormalization(256)\n",
        "        # relu\n",
        "        self.refpad04_2 = nn.ReflectionPad2d(1)\n",
        "        self.conv04_2 = nn.Conv2d(256, 256, 3)\n",
        "        self.in04_2 = InstanceNormalization(256)\n",
        "        # + input\n",
        "\n",
        "        # res block 2\n",
        "        self.refpad05_1 = nn.ReflectionPad2d(1)\n",
        "        self.conv05_1 = nn.Conv2d(256, 256, 3)\n",
        "        self.in05_1 = InstanceNormalization(256)\n",
        "        # relu\n",
        "        self.refpad05_2 = nn.ReflectionPad2d(1)\n",
        "        self.conv05_2 = nn.Conv2d(256, 256, 3)\n",
        "        self.in05_2 = InstanceNormalization(256)\n",
        "        # + input\n",
        "\n",
        "        # res block 3\n",
        "        self.refpad06_1 = nn.ReflectionPad2d(1)\n",
        "        self.conv06_1 = nn.Conv2d(256, 256, 3)\n",
        "        self.in06_1 = InstanceNormalization(256)\n",
        "        # relu\n",
        "        self.refpad06_2 = nn.ReflectionPad2d(1)\n",
        "        self.conv06_2 = nn.Conv2d(256, 256, 3)\n",
        "        self.in06_2 = InstanceNormalization(256)\n",
        "        # + input\n",
        "\n",
        "        # res block 4\n",
        "        self.refpad07_1 = nn.ReflectionPad2d(1)\n",
        "        self.conv07_1 = nn.Conv2d(256, 256, 3)\n",
        "        self.in07_1 = InstanceNormalization(256)\n",
        "        # relu\n",
        "        self.refpad07_2 = nn.ReflectionPad2d(1)\n",
        "        self.conv07_2 = nn.Conv2d(256, 256, 3)\n",
        "        self.in07_2 = InstanceNormalization(256)\n",
        "        # + input\n",
        "\n",
        "        # res block 5\n",
        "        self.refpad08_1 = nn.ReflectionPad2d(1)\n",
        "        self.conv08_1 = nn.Conv2d(256, 256, 3)\n",
        "        self.in08_1 = InstanceNormalization(256)\n",
        "        # relu\n",
        "        self.refpad08_2 = nn.ReflectionPad2d(1)\n",
        "        self.conv08_2 = nn.Conv2d(256, 256, 3)\n",
        "        self.in08_2 = InstanceNormalization(256)\n",
        "        # + input\n",
        "\n",
        "        # res block 6\n",
        "        self.refpad09_1 = nn.ReflectionPad2d(1)\n",
        "        self.conv09_1 = nn.Conv2d(256, 256, 3)\n",
        "        self.in09_1 = InstanceNormalization(256)\n",
        "        # relu\n",
        "        self.refpad09_2 = nn.ReflectionPad2d(1)\n",
        "        self.conv09_2 = nn.Conv2d(256, 256, 3)\n",
        "        self.in09_2 = InstanceNormalization(256)\n",
        "        # + input\n",
        "\n",
        "        # res block 7\n",
        "        self.refpad10_1 = nn.ReflectionPad2d(1)\n",
        "        self.conv10_1 = nn.Conv2d(256, 256, 3)\n",
        "        self.in10_1 = InstanceNormalization(256)\n",
        "        # relu\n",
        "        self.refpad10_2 = nn.ReflectionPad2d(1)\n",
        "        self.conv10_2 = nn.Conv2d(256, 256, 3)\n",
        "        self.in10_2 = InstanceNormalization(256)\n",
        "        # + input\n",
        "\n",
        "        # res block 8\n",
        "        self.refpad11_1 = nn.ReflectionPad2d(1)\n",
        "        self.conv11_1 = nn.Conv2d(256, 256, 3)\n",
        "        self.in11_1 = InstanceNormalization(256)\n",
        "        # relu\n",
        "        self.refpad11_2 = nn.ReflectionPad2d(1)\n",
        "        self.conv11_2 = nn.Conv2d(256, 256, 3)\n",
        "        self.in11_2 = InstanceNormalization(256)\n",
        "        # + input\n",
        "\n",
        "        ##------------------------------------##\n",
        "        self.deconv01_1 = nn.ConvTranspose2d(256, 128, 3, 2, 1, 1)\n",
        "        self.deconv01_2 = nn.Conv2d(128, 128, 3, 1, 1)\n",
        "        self.in12_1 = InstanceNormalization(128)\n",
        "        # relu\n",
        "        self.deconv02_1 = nn.ConvTranspose2d(128, 64, 3, 2, 1, 1)\n",
        "        self.deconv02_2 = nn.Conv2d(64, 64, 3, 1, 1)\n",
        "        self.in13_1 = InstanceNormalization(64)\n",
        "        # relu\n",
        "        self.refpad12_1 = nn.ReflectionPad2d(3)\n",
        "        self.deconv03_1 = nn.Conv2d(64, 3, 7)\n",
        "        # tanh\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.relu(self.in01_1(self.conv01_1(self.refpad01_1(x))))\n",
        "        y = F.relu(self.in02_1(self.conv02_2(self.conv02_1(y))))\n",
        "        t04 = F.relu(self.in03_1(self.conv03_2(self.conv03_1(y))))\n",
        "\n",
        "        ##\n",
        "        y = F.relu(self.in04_1(self.conv04_1(self.refpad04_1(t04))))\n",
        "        t05 = self.in04_2(self.conv04_2(self.refpad04_2(y))) + t04\n",
        "\n",
        "        y = F.relu(self.in05_1(self.conv05_1(self.refpad05_1(t05))))\n",
        "        t06 = self.in05_2(self.conv05_2(self.refpad05_2(y))) + t05\n",
        "\n",
        "        y = F.relu(self.in06_1(self.conv06_1(self.refpad06_1(t06))))\n",
        "        t07 = self.in06_2(self.conv06_2(self.refpad06_2(y))) + t06\n",
        "\n",
        "        y = F.relu(self.in07_1(self.conv07_1(self.refpad07_1(t07))))\n",
        "        t08 = self.in07_2(self.conv07_2(self.refpad07_2(y))) + t07\n",
        "\n",
        "        y = F.relu(self.in08_1(self.conv08_1(self.refpad08_1(t08))))\n",
        "        t09 = self.in08_2(self.conv08_2(self.refpad08_2(y))) + t08\n",
        "\n",
        "        y = F.relu(self.in09_1(self.conv09_1(self.refpad09_1(t09))))\n",
        "        t10 = self.in09_2(self.conv09_2(self.refpad09_2(y))) + t09\n",
        "\n",
        "        y = F.relu(self.in10_1(self.conv10_1(self.refpad10_1(t10))))\n",
        "        t11 = self.in10_2(self.conv10_2(self.refpad10_2(y))) + t10\n",
        "\n",
        "        y = F.relu(self.in11_1(self.conv11_1(self.refpad11_1(t11))))\n",
        "        y = self.in11_2(self.conv11_2(self.refpad11_2(y))) + t11\n",
        "        ##\n",
        "\n",
        "        y = F.relu(self.in12_1(self.deconv01_2(self.deconv01_1(y))))\n",
        "        y = F.relu(self.in13_1(self.deconv02_2(self.deconv02_1(y))))\n",
        "        y = torch.tanh(self.deconv03_1(self.refpad12_1(y)))\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class InstanceNormalization(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-9):\n",
        "        super(InstanceNormalization, self).__init__()\n",
        "        self.scale = nn.Parameter(torch.FloatTensor(dim))\n",
        "        self.shift = nn.Parameter(torch.FloatTensor(dim))\n",
        "        self.eps = eps\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        self.scale.data.uniform_()\n",
        "        self.shift.data.zero_()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        n = x.size(2) * x.size(3)\n",
        "        t = x.view(x.size(0), x.size(1), n)\n",
        "        mean = torch.mean(t, 2).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "        # Calculate the biased var. torch.var returns unbiased var\n",
        "        var = torch.std(t, 2) ** 2\n",
        "        var = var.unsqueeze(2).unsqueeze(3).expand_as(x) * (\n",
        "            (n - 1) / torch.FloatTensor([n])\n",
        "        )\n",
        "        scale_broadcast = self.scale.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n",
        "        scale_broadcast = scale_broadcast.expand_as(x)\n",
        "        shift_broadcast = self.shift.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n",
        "        shift_broadcast = shift_broadcast.expand_as(x)\n",
        "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        out = out * scale_broadcast + shift_broadcast\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123,
          "referenced_widgets": [
            "e68aeea0d3e04a7ca469270220d600ce",
            "9f6335cf731b4ee0b8c678b561dd9790",
            "256468b01c354c3588359dcf9702e3a1",
            "211f1b68afa04bd093dfddcc2875b708",
            "15196c099f1443548590ef5bc7f14fc6",
            "db3d2525b05b4feea723561b1541e662",
            "5c5021e596da4297aa69f1d57972e3c8",
            "8ef9b83e99b34f69a22f4371d88a6eb3",
            "d80adb7f97614194805a1f506ad7b8b9",
            "ebaf268e5736486092c073a298b89601",
            "1191dbbe06254c18a0aeb9da1f5e696f"
          ]
        },
        "id": "-mKrZ2yOpLbM",
        "outputId": "f1143f17-11d5-4a4e-919e-6483f980f898"
      },
      "outputs": [],
      "source": [
        "styles = [\"Hosoda\", \"Hayao\", \"Shinkai\", \"Paprika\"]\n",
        "\n",
        "models = {}\n",
        "\n",
        "for style in styles:\n",
        "  model = Transformer()\n",
        "  model.load_state_dict(torch.load(os.path.join(\"./pretrained_models\", style + '_net_G_float.pth')))\n",
        "  model.eval()\n",
        "  models[style] = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex5-gXDP4sgz",
        "outputId": "8833ab4d-c0d2-4069-9280-71b56e3a417a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--gpu'], dest='gpu', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# style = \"Hayao\"\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--input_dir', default = 'test_images')\n",
        "parser.add_argument('--load_size', default = 756)\n",
        "parser.add_argument('--model_path', default = './pretrained_model')\n",
        "parser.add_argument('--style', default = 'Hayao')\n",
        "parser.add_argument('--output_dir', default = 'cartoonized_images')\n",
        "parser.add_argument('--gpu', type=int, default = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Namespace(input_dir='test_images', load_size=756, model_path='./pretrained_model', style='Hayao', output_dir='cartoonized_images', gpu=0)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "opt, unknown = parser.parse_known_args()\n",
        "opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "for files in os.listdir(opt.input_dir):\n",
        "\text = os.path.splitext(files)[1]\n",
        "\tif ext not in ['.jpg', '.png']:\n",
        "\t\tcontinue\n",
        "\t\n",
        "    # load image\n",
        "\tinput_image = Image.open(os.path.join(opt.input_dir, files)).convert(\"RGB\")\n",
        "\t\n",
        "\t# resize image, keep aspect ratio\n",
        "\th = input_image.size[0]\n",
        "\tw = input_image.size[1]\n",
        "\tratio = h *1.0 / w\n",
        "\tif ratio > 1:\n",
        "\t\th = opt.load_size\n",
        "\t\tw = int(h*1.0/ratio)\n",
        "\telse:\n",
        "\t\tw = opt.load_size\n",
        "\t\th = int(w * ratio)\n",
        "\tinput_image = input_image.resize((h, w), Image.BICUBIC)\n",
        "\tinput_image = np.asarray(input_image)\n",
        "\t\n",
        "\t# RGB -> BGR\n",
        "\tinput_image = input_image[:, :, [2, 1, 0]]\n",
        "\tinput_image = transforms.ToTensor()(input_image).unsqueeze(0)\n",
        "\t\n",
        "    # preprocess, (-1, 1)\n",
        "\tinput_image = -1 + 2 * input_image \n",
        "\t\n",
        "    # if opt.gpu > -1:\n",
        "\t# \tinput_image = Variable(input_image, volatile=True).cuda()\n",
        "\t# else:\n",
        "\t# \tinput_image = Variable(input_image, volatile=True).float()\n",
        "\t\n",
        "    # forward\n",
        "\toutput_image = models[opt.style](input_image)\n",
        "\toutput_image = output_image[0]\n",
        "\t# BGR -> RGB\n",
        "\toutput_image = output_image[[2, 1, 0], :, :]\n",
        "\t# deprocess, (0, 1)\n",
        "\toutput_image = output_image.data.cpu().float() * 0.5 + 0.5\n",
        "\t\n",
        "\t# save\n",
        "\tvutils.save_image(output_image, os.path.join(opt.output_dir, files[:-4] + '_' + opt.style + '.jpg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "import base64, io\n",
        "\n",
        "img_path = os.path.join('test_images', \"test.png\")\n",
        "input_image = Image.open(img_path)\n",
        "\n",
        "extension = input_image.format\n",
        "\n",
        "input_image.convert(\"RGB\")\n",
        "\n",
        "# Create in-memory file from the PIL image\n",
        "in_mem_file = io.BytesIO()\n",
        "input_image.save(in_mem_file, format = extension)\n",
        "\n",
        "# Read the bytes\n",
        "in_mem_file.seek(0)\n",
        "image_bytes = in_mem_file.read()\n",
        "\n",
        "base64_string = base64.b64encode(image_bytes).decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "f = open(\"demofile.txt\", \"r\")\n",
        "base64_string = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "from base64 import b64decode\n",
        "from PIL import Image\n",
        "import imghdr, io\n",
        "\n",
        "decoded_string = b64decode(base64_string)\n",
        "extension = imghdr.what(None, h=decoded_string)\n",
        "print(extension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file <_io.BytesIO object at 0x000001FB5BBC6E30>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded_string\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32md:\\Desktop\\Master_Degree\\Works\\Cloud Computing\\FinalProject\\CartoonGAN\\cartoonGAN\\lib\\site-packages\\PIL\\Image.py:3309\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3307\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message)\n\u001b[0;32m   3308\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot identify image file \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (filename \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;28;01melse\u001b[39;00m fp)\n\u001b[1;32m-> 3309\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
            "\u001b[1;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x000001FB5BBC6E30>"
          ]
        }
      ],
      "source": [
        "input_image = Image.open(io.BytesIO(decoded_string)).convert(\"RGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_image.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LOAD_SIZE = 756\n",
        "import base64, io\n",
        "\n",
        "def predict_fn(input_data, model):\n",
        "    # logger.info('Generating cartoonization.')\n",
        "\t\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\t\n",
        "    imgdata = base64.b64decode(input_data)\n",
        "    input_image = Image.open(io.BytesIO(imgdata)).convert(\"RGB\")\n",
        "\n",
        "    # logger.info('Valid datatype')\n",
        "    # resize image, keep aspect ratio\n",
        "    # h = input_image.size[0]\n",
        "    # w = input_image.size[1]\n",
        "    # ratio = h *1.0 / w\n",
        "    # if ratio > 1:\n",
        "    #     h = LOAD_SIZE\n",
        "    #     w = int(h*1.0/ratio)\n",
        "    # else:\n",
        "    #     w = LOAD_SIZE\n",
        "    #     h = int(w * ratio)\n",
        "    # input_image = input_image.resize((h, w), Image.BICUBIC)\n",
        "    input_image = np.asarray(input_image)\n",
        "\n",
        "    # RGB -> BGR\n",
        "    input_image = input_image[:, :, [2, 1, 0]]\n",
        "    input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n",
        "\t\n",
        "    # preprocess, (-1, 1)\n",
        "    input_image = -1 + 2 * input_image \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        input_image = input_image.to(device)\n",
        "        cartoonized = model(input_image)\n",
        "\n",
        "    output_image = cartoonized[0]\n",
        "\t# BGR -> RGB\n",
        "    output_image = output_image[[2, 1, 0], :, :]\n",
        "\t# deprocess, (0, 1)\n",
        "    output_image = output_image.data.cpu().float() * 0.5 + 0.5\n",
        "\n",
        "    # logger.info('Complete prediction')\n",
        "    return output_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = predict_fn(base64_string, models[opt.style])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "vutils.save_image(output, f'test.{extension}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def output_fn(prediction, response_content_type):\n",
        "    # logger.info(f'Generating output. {prediction}')\n",
        "    if response_content_type == 'application/json':\n",
        "        transform = transforms.ToPILImage(\"RGB\")\n",
        "        pil_image = transform(prediction)\n",
        "\n",
        "        # Create in-memory file from the PIL image\n",
        "        in_mem_file = io.BytesIO()\n",
        "        pil_image.save(in_mem_file, format = \"PNG\")\n",
        "\n",
        "        # Read the bytes\n",
        "        in_mem_file.seek(0)\n",
        "        image_bytes = in_mem_file.read()\n",
        "\n",
        "        # Convert the PIL image to bytes\n",
        "        # image_bytes = pil_image.getvalue()\n",
        "\n",
        "        # Encode the bytes to base64\n",
        "        base64_string = base64.b64encode(image_bytes).decode('utf-8')\n",
        "        # logger.info(f'Output generated {base64_string}')\n",
        "\n",
        "        return base64_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "test = output_fn(output, 'application/json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "f = open(\"demofile3.txt\", \"w\")\n",
        "f.write(test)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare the model for compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tarfile, os\n",
        "with tarfile.open('hayao_model.tar.gz', 'w:gz') as f:\n",
        "    f.add(\"hayao_model.pth\")\n",
        "    f.add(\"code\", arcname=os.path.basename(\"code\"))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'code'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.basename(\"./code\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_archive_name = \"hayao_model.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "a ./hayao_model.pth\n",
            "a ./code\n",
            "a ./code/serve.py\n"
          ]
        }
      ],
      "source": [
        "!tar -cvpzf {model_archive_name} \"./hayao_model.pth\" ./code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1191dbbe06254c18a0aeb9da1f5e696f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15196c099f1443548590ef5bc7f14fc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "211f1b68afa04bd093dfddcc2875b708": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebaf268e5736486092c073a298b89601",
            "placeholder": "​",
            "style": "IPY_MODEL_1191dbbe06254c18a0aeb9da1f5e696f",
            "value": " 4/4 [00:00&lt;00:00,  6.28it/s]"
          }
        },
        "256468b01c354c3588359dcf9702e3a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ef9b83e99b34f69a22f4371d88a6eb3",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d80adb7f97614194805a1f506ad7b8b9",
            "value": 4
          }
        },
        "5c5021e596da4297aa69f1d57972e3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ef9b83e99b34f69a22f4371d88a6eb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f6335cf731b4ee0b8c678b561dd9790": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db3d2525b05b4feea723561b1541e662",
            "placeholder": "​",
            "style": "IPY_MODEL_5c5021e596da4297aa69f1d57972e3c8",
            "value": "100%"
          }
        },
        "d80adb7f97614194805a1f506ad7b8b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db3d2525b05b4feea723561b1541e662": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e68aeea0d3e04a7ca469270220d600ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f6335cf731b4ee0b8c678b561dd9790",
              "IPY_MODEL_256468b01c354c3588359dcf9702e3a1",
              "IPY_MODEL_211f1b68afa04bd093dfddcc2875b708"
            ],
            "layout": "IPY_MODEL_15196c099f1443548590ef5bc7f14fc6"
          }
        },
        "ebaf268e5736486092c073a298b89601": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
